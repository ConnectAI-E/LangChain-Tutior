
 
1
00:00:01,000 --> 00:00:15,000
使用llm构建的最常见的复杂应用程序之一是一个系统，可以在文档上方或关于文档回答问题。

2
00:00:15,000 --> 00:00:24,000
因此，给定从PDF文件、网页或某些公司的内部文档收集中提取的文本，

3
00:00:24,000 --> 00:00:33,000
您可以使用llm回答有关这些文档内容的问题，以帮助用户获得更深入的理解并获得所需的信息吗？

4
00:00:33,000 --> 00:00:39,000
这真的很强大，因为它开始将这些语言模型与它们最初没有接受培训的数据结合起来。

5
00:00:39,000 --> 00:00:42,000
因此，它使它们更加灵活和适应您的用例。

6
00:00:42,000 --> 00:00:48,000
这也非常令人兴奋，因为我们将开始超越语言模型、提示和输出解析器，

7
00:00:48,000 --> 00:00:54,000
并开始引入链式的一些关键组件，例如嵌入模型和向量存储。

8
00:00:54,000 --> 00:00:58,000
正如安德鲁所提到的，这是我们拥有的最受欢迎的链之一，所以我希望你很兴奋。

9
00:00:58,000 --> 00:01:03,000
实际上，嵌入和向量存储是一些最强大的现代技术。

10
00:01:03,000 --> 00:01:08,000
因此，如果您还没有看到它们，那么了解它们非常值得。

11
00:01:08,000 --> 00:01:10,000
那么，让我们开始吧。

12
00:01:10,000 --> 00:01:11,000
开始吧。

13
00:01:11,000 --> 00:01:16,000
因此，我们将从像往常一样导入环境变量开始。

14
00:01:16,000 --> 00:01:20,000
现在，我们将导入一些在构建此链时将有所帮助的东西。

15
00:01:20,000 --> 00:01:22,000
我们将导入检索QA链。

16
00:01:22,000 --> 00:01:24,000
这将在一些文档上进行检索。

17
00:01:24,000 --> 00:01:28,000
我们将导入我们最喜欢的聊天Open AI语言模型。

18
00:01:28,000 --> 00:01:29,000
我们将导入文档加载器。

19
00:01:29,000 --> 00:01:34,000
这将用于加载一些专有数据，我们将与语言模型结合使用。

20
00:01:34,000 --> 00:01:36,000
在这种情况下，它将在CSV中。

21
00:01:36,000 --> 00:01:39,000
因此，我们将导入CSV加载器。

22
00:01:39,000 --> 00:01:41,000
最后，我们将导入向量存储。

23
00:01:41,000 --> 00:01:45,000
有许多不同类型的向量存储，我们将在稍后介绍它们的确切含义。

24
00:01:45,000 --> 00:01:49,000
但是，我们将从Dock Array内存搜索向量存储开始。

25
00:01:49,000 --> 00:01:51,000
这非常好，因为它是一个内存向量存储，

26
00:01:51,000 --> 00:01:55,000
并且不需要连接到任何外部数据库，
 
27
00:01:55,000 --> 00:01:57,000
所以它使得入门变得非常容易。

28
00:01:57,000 --> 00:01:59,000
我们还将导入显示和markdown两个常见的在Jupyter Notebooks中显示信息的工具。

29
00:01:59,000 --> 00:02:04,000
我们提供了一个户外服装的CSV文件，我们将使用它与语言模型结合使用。

30
00:02:04,000 --> 00:02:10,000
在这里，我们将使用该文件的路径初始化一个加载器，即CSV加载器。

31
00:02:10,000 --> 00:02:18,000
接下来，我们将导入一个索引，即向量存储索引创建器。

32
00:02:18,000 --> 00:02:22,000
这将帮助我们非常容易地创建一个向量存储。

33
00:02:22,000 --> 00:02:26,000
如下所示，只需要几行代码就可以创建它。

34
00:02:26,000 --> 00:02:34,000
为了创建它，我们将指定两件事。

35
00:02:34,000 --> 00:02:37,000
首先，我们将指定向量存储类。

36
00:02:37,000 --> 00:02:40,000
如前所述，我们将使用这个向量存储，

37
00:02:40,000 --> 00:02:46,000
因为它是一个特别容易入门的向量存储。

38
00:02:46,000 --> 00:02:49,000
创建完成后，我们将从加载器中调用，

39
00:02:49,000 --> 00:02:51,000
它接受一个文档加载器列表。

40
00:02:51,000 --> 00:02:58,000
我们只有一个我们真正关心的加载器，所以这就是我们在这里传递的。

41
00:02:58,000 --> 00:03:02,000
现在它已经被创建了，我们可以开始询问它的问题了。

42
00:03:02,000 --> 00:03:07,000
下面我们将介绍发生了什么，所以现在不要担心这个。

43
00:03:07,000 --> 00:03:09,000
在这里我们将从一个查询开始。

44
00:03:09,000 --> 00:03:17,000
然后我们将使用索引查询创建一个响应，并传入这个查询。

45
00:03:17,000 --> 00:03:21,000
同样，我们将在下面介绍发生了什么。

46
00:03:21,000 --> 00:03:30,000
现在，我们只需要等待它的响应。

47
00:03:30,000 --> 00:03:34,000
完成后，我们现在可以看看到底返回了什么。

48
00:03:34,000 --> 00:03:41,000
我们得到了一个Markdown表格，其中包含所有带有防晒衣的衬衫的名称和描述。

49
00:03:41,000 --> 00:03:45,000
我们还得到了一个语言模型提供的不错的小总结。

50
00:03:45,000 --> 00:03:48,000
所以我们已经介绍了如何在您的文档中进行问答，

51
00:03:48,000 --> 00:03:52,000
但是到底在幕后发生了什么呢？

52
00:03:52,000 --> 00:03:54,000
首先，让我们考虑一般的想法。

53
00:03:54,000 --> 00:03:58,000
我们想要使用语言模型并将其与我们的许多文档结合使用，

54
00:03:58,000 --> 00:04:03,000
但是有一个关键问题。语言模型一次只能检查几千个单词。
 
56
00:04:03,000 --> 00:04:10,000
如果我们有非常大的文档，如何让语言模型回答关于其中所有内容的问题呢？

57
00:04:10,000 --> 00:04:14,000
这就是嵌入和向量存储发挥作用的地方。

58
00:04:14,000 --> 00:04:17,000
首先，让我们谈谈嵌入。

59
00:04:17,000 --> 00:04:21,000
嵌入为文本片段创建数字表示。

60
00:04:21,000 --> 00:04:27,000
这种数字表示捕捉了它所运行的文本片段的语义含义。

61
00:04:27,000 --> 00:04:31,000
相似内容的文本片段将具有相似的向量。

62
00:04:31,000 --> 00:04:35,000
这使我们可以在向量空间中比较文本片段。

63
00:04:35,000 --> 00:04:38,000
在下面的示例中，我们可以看到我们有三个句子。

64
00:04:38,000 --> 00:04:43,000
前两个是关于宠物的，而第三个是关于汽车的。

65
00:04:43,000 --> 00:04:46,000
如果我们看一下数字空间中的表示，

66
00:04:46,000 --> 00:04:54,000
我们可以看到当我们比较与宠物句子相对应的文本片段上的两个向量时，它们非常相似。

67
00:04:54,000 --> 00:04:58,000
而如果我们将其与谈论汽车的那个进行比较，它们根本不相似。

68
00:04:58,000 --> 00:05:02,000
这将让我们轻松地找出哪些文本片段彼此相似，

69
00:05:02,000 --> 00:05:10,000
这在我们考虑要包含哪些文本片段传递给语言模型以回答问题时非常有用。

70
00:05:10,000 --> 00:05:13,000
我们要介绍的下一个组件是向量数据库。

71
00:05:13,000 --> 00:05:18,000
向量数据库是存储我们在上一步中创建的这些向量表示的一种方式。

72
00:05:18,000 --> 00:05:24,000
我们创建这个向量数据库的方式是用来自传入文档的文本块填充它。

73
00:05:24,000 --> 00:05:28,000
当我们获得一个大的传入文档时，我们首先将其分成较小的块。

74
00:05:28,000 --> 00:05:33,000
这有助于创建比原始文档小的文本片段，

75
00:05:33,000 --> 00:05:37,000
这很有用，因为我们可能无法将整个文档传递给语言模型。

76
00:05:37,000 --> 00:05:43,000
因此，我们想创建这些小块，以便只传递最相关的块给语言模型。

77
00:05:43,000 --> 00:05:48,000
然后，我们为每个这些块创建一个嵌入，然后将它们存储在向量数据库中。

78
00:05:48,000 --> 00:05:51,000
这就是我们创建索引时发生的事情。

79
00:05:51,000 --> 00:05:58,000
现在我们有了这个索引，我们可以在运行时使用它来查找与传入查询最相关的文本片段。

80
00:05:58,000 --> 00:06:02,000
当查询进来时，我们首先为该查询创建一个嵌入。
 
81
00:06:02,000 --> 00:06:07,000
然后我们将其与向量数据库中的所有向量进行比较，并选择最相似的n个。

82
00:06:07,000 --> 00:06:14,000
然后将它们返回，我们可以将它们传递到语言模型中，以获得最终答案。

83
00:06:14,000 --> 00:06:17,000
因此，我们创建了这个链，只需要几行代码。

84
00:06:17,000 --> 00:06:19,000
这对于快速入门非常有用。

85
00:06:19,000 --> 00:06:25,000
好的，现在让我们逐步进行，并了解底层到底发生了什么。

86
00:06:25,000 --> 00:06:27,000
第一步与上面类似。

87
00:06:27,000 --> 00:06:36,000
我们将创建一个文档加载器，从包含我们要进行问题回答的所有产品描述的CSV中加载。

88
00:06:36,000 --> 00:06:41,000
然后我们可以从这个文档加载器中加载文档。

89
00:06:41,000 --> 00:06:50,000
如果我们查看单个文档，我们可以看到每个文档对应于CSV中的一个产品。

90
00:06:50,000 --> 00:06:53,000
之前，我们谈到了创建块。

91
00:06:53,000 --> 00:07:01,000
因为这些文档已经非常小了，所以我们实际上不需要在这里进行任何分块，因此我们可以直接创建嵌入。

92
00:07:01,000 --> 00:07:05,000
要创建嵌入，我们将使用OpenAI的嵌入类。

93
00:07:05,000 --> 00:07:08,000
我们可以在这里导入它并初始化它。

94
00:07:08,000 --> 00:07:21,000
如果我们想看看这些嵌入是如何工作的，我们实际上可以看一下嵌入特定文本时会发生什么。

95
00:07:21,000 --> 00:07:26,000
让我们使用嵌入对象上的嵌入查询方法为特定文本创建嵌入。

96
00:07:26,000 --> 00:07:31,000
在这种情况下，句子是“嗨，我的名字是哈里森”。

97
00:07:31,000 --> 00:07:41,000
如果我们查看这个嵌入，我们可以看到有超过一千个不同的元素。

98
00:07:41,000 --> 00:07:44,000
每个元素都是不同的数字值。

99
00:07:44,000 --> 00:07:51,000
组合起来，这就创建了这段文本的总体数值表示。

100
00:07:51,000 --> 00:07:58,000
我们想为刚刚加载的所有文本创建嵌入，然后我们还想将它们存储在向量存储中。

101
00:07:58,000 --> 00:08:03,000
我们可以使用向量存储上的from documents方法来实现这一点。

102
00:08:03,000 --> 00:08:12,000
该方法接受文档列表、嵌入对象，然后我们将创建一个总体向量存储。

103
00:08:12,000 --> 00:08:18,000
现在，我们可以使用这个向量存储来查找与传入查询类似的文本。

104
00:08:18,000 --> 00:08:23,000
因此，让我们看一下查询，请建议一件带有防晒功能的衬衫。
 
105
00:08:23,000 --> 00:08:36,000
如果我们在向量存储中使用相似性搜索方法并传入一个查询，我们将得到一个文档列表。

106
00:08:36,000 --> 00:08:48,000
我们可以看到它返回了四个文档，如果我们看第一个文档，我们可以看到它确实是一件关于防晒的衬衫。

107
00:08:48,000 --> 00:08:52,000
那么我们如何使用它来回答我们自己的文档问题呢？

108
00:08:52,000 --> 00:08:57,000
首先，我们需要从这个向量存储中创建一个检索器。

109
00:08:57,000 --> 00:09:03,000
检索器是一个通用接口，可以由任何接受查询并返回文档的方法支持。

110
00:09:03,000 --> 00:09:11,000
向量存储和嵌入是一种这样的方法，尽管有许多不同的方法，有些不太先进，有些更先进。

111
00:09:11,000 --> 00:09:20,000
接下来，因为我们想要进行文本生成并返回自然语言响应，我们将导入一个语言模型，我们将使用聊天开放AI。

112
00:09:20,000 --> 00:09:28,000
如果我们手动进行此操作，我们将合并文档中的所有页面内容到一个变量中。

113
00:09:28,000 --> 00:09:37,000
因此，我们会做一些像这样的事情，将所有页面内容连接到一个变量中。

114
00:09:37,000 --> 00:09:48,000
然后，我们将传递此变量或问题的变体，例如请列出所有具有防晒功能的衬衫并在Markdown表格中总结每个衬衫的语言模型。

115
00:09:48,000 --> 00:09:55,000
如果我们在此处打印响应，我们可以看到我们得到了一个表格，正如我们所要求的那样。

116
00:09:55,000 --> 00:09:59,000
所有这些步骤都可以用LangChain链封装起来。

117
00:09:59,000 --> 00:10:02,000
因此，我们可以创建一个检索QA链。

118
00:10:02,000 --> 00:10:06,000
这将进行检索，然后对检索到的文档进行问题回答。

119
00:10:06,000 --> 00:10:09,000
要创建这样的链，我们将传入几个不同的东西。

120
00:10:09,000 --> 00:10:12,000
首先，我们将传入语言模型。

121
00:10:12,000 --> 00:10:15,000
这将用于在最后进行文本生成。

122
00:10:15,000 --> 00:10:17,000
接下来，我们将传入链类型。

123
00:10:17,000 --> 00:10:18,000
我们将使用stuff。

124
00:10:18,000 --> 00:10:25,000
这是最简单的方法，因为它只是将所有文档塞入上下文并对语言模型进行一次调用。

125
00:10:25,000 --> 00:10:32,000
还有一些其他方法可以用来进行问题回答，我可能会在最后提及，但我们不会详细讨论。

126
00:10:32,000 --> 00:10:34,000
第三，我们将传入一个检索器。

127
00:10:34,000 --> 00:10:38,000
我们上面创建的检索器只是一个获取文档的接口。
 
128
00:10:38,000 --> 00:10:41,000
这将用于获取文档并将其传递给语言模型。

129
00:10:41,000 --> 00:10:46,000
最后，我们将设置 verbose 等于 true。

130
00:10:46,000 --> 00:11:08,000
现在我们可以创建一个查询并在此查询上运行链。

131
00:11:08,000 --> 00:11:14,000
当我们获得响应时，我们可以再次使用 display 和 markdown 实用程序显示它。

132
00:11:14,000 --> 00:11:20,000
您可以在此暂停视频并尝试使用一堆不同的查询。

133
00:11:20,000 --> 00:11:26,000
所以这就是您详细了解它的方式，但请记住，我们仍然可以轻松地使用我们上面的一行来完成它。

134
00:11:26,000 --> 00:11:30,000
因此，这两个东西等同于相同的结果。

135
00:11:30,000 --> 00:11:32,000
这就是 LinkChain 的有趣之处。

136
00:11:32,000 --> 00:11:38,000
您可以在一行中完成它，也可以查看各个内容并将其分解为更详细的五个内容。

137
00:11:38,000 --> 00:11:44,000
五个更详细的内容让您设置更多关于正在发生的确切内容的细节，但一行代码很容易入手。

138
00:11:44,000 --> 00:11:48,000
所以由您决定如何继续前进。

139
00:11:48,000 --> 00:11:51,000
我们还可以在创建索引时自定义索引。

140
00:11:51,000 --> 00:11:55,000
因此，如果您记得，当我们手动创建它时，我们指定了一个嵌入。

141
00:11:55,000 --> 00:11:57,000
我们也可以在这里指定一个嵌入。

142
00:11:57,000 --> 00:12:01,000
这将使我们能够灵活地创建嵌入本身。

143
00:12:01,000 --> 00:12:06,000
我们还可以在此处替换向量存储器以获取不同类型的向量存储器。

144
00:12:06,000 --> 00:12:15,000
因此，在创建索引时，您可以进行与手动创建时相同级别的自定义。

145
00:12:15,000 --> 00:12:17,000
在这个笔记本中，我们使用了 stuff 方法。

146
00:12:17,000 --> 00:12:19,000
stuff 方法非常好，因为它非常简单。

147
00:12:19,000 --> 00:12:25,000
您只需将所有内容放入一个提示符中，然后将其发送到语言模型并获取一个响应。

148
00:12:25,000 --> 00:12:27,000
因此，很容易理解正在发生什么。

149
00:12:27,000 --> 00:12:30,000
它非常便宜，而且效果很好。

150
00:12:30,000 --> 00:12:32,000
但是，这并不总是可以正常工作。

151
00:12:32,000 --> 00:12:37,000
因此，如果您记得，在笔记本中获取文档时，我们只返回了四个文档。

152
00:12:37,000 --> 00:12:39,000
它们相对较小。

153
00:12:39,000 --> 00:12:44,000
但是，如果您想在许多不同类型的块上执行相同类型的问答，该怎么办？

154
00:12:44,000 --> 00:12:47,000
那么我们可以使用几种不同的方法。
 
155
00:12:47,000 --> 00:12:48,000
第一个是Map Reduce。

156
00:12:48,000 --> 00:12:55,000
这基本上是将所有块与问题一起传递给语言模型，获取回复，

157
00:12:55,000 --> 00:13:02,000
然后使用另一个语言模型调用将所有单独的回复总结成最终答案。

158
00:13:02,000 --> 00:13:06,000
这非常强大，因为它可以在任意数量的文档上运行。

159
00:13:06,000 --> 00:13:11,000
而且它也非常强大，因为您可以并行处理单个问题。

160
00:13:11,000 --> 00:13:13,000
但是它需要更多的调用。

161
00:13:13,000 --> 00:13:19,000
它将所有文档视为独立的，这可能并不总是最理想的事情。

162
00:13:19,000 --> 00:13:24,000
Refine是另一种方法，再次用于循环许多文档。

163
00:13:24,000 --> 00:13:25,000
但它实际上是迭代的。

164
00:13:25,000 --> 00:13:28,000
它建立在先前文档的答案之上。

165
00:13:28,000 --> 00:13:33,000
因此，这非常适合组合信息并随时间逐步构建答案。

166
00:13:33,000 --> 00:13:36,000
它通常会导致更长的答案。

167
00:13:36,000 --> 00:13:39,000
而且它也不太快，因为现在调用不是独立的。

168
00:13:39,000 --> 00:13:43,000
它们依赖于先前调用的结果。

169
00:13:43,000 --> 00:13:49,000
这意味着它通常需要更长的时间，并且基本上需要与Map Reduce一样多的调用。

170
00:13:49,000 --> 00:13:57,000
Map Re-rank是一种相当有趣且更为实验性的方法，其中您对每个文档进行单个语言模型调用。

171
00:13:57,000 --> 00:14:00,000
然后您还要求它返回一个分数。

172
00:14:00,000 --> 00:14:02,000
然后您选择最高分。

173
00:14:02,000 --> 00:14:06,000
这依赖于语言模型知道分数应该是什么。

174
00:14:06,000 --> 00:14:12,000
因此，您经常需要告诉它，嘿，如果它与文档相关，则应该是高分，并在那里精细调整说明。

175
00:14:12,000 --> 00:14:15,000
与Map Reduce类似，所有调用都是独立的。

176
00:14:15,000 --> 00:14:16,000
所以您可以批量处理它们。

177
00:14:16,000 --> 00:14:18,000
而且它相对较快。

178
00:14:18,000 --> 00:14:20,000
但是，您正在进行大量的语言模型调用。

179
00:14:20,000 --> 00:14:22,000
因此，它会更加昂贵。

180
00:14:22,000 --> 00:14:29,000
这些方法中最常见的是Stuff方法，我们在笔记本中使用它将所有内容组合成一个文档。

181
00:14:29,000 --> 00:14:35,000
第二种最常见的方法是Map Reduce方法，它将这些块发送到语言模型。
 
182
00:14:35,000 --> 00:14:42,000
这里的这些方法，如stuff、map reduce、refine和re-rank，也可以用于除了问答之外的许多其他链。

183
00:14:42,000 --> 00:14:53,000
例如，map reduce链的一个非常常见的用例是摘要，其中您有一个非常长的文档，您想要递归地摘要其中的信息片段。

184
00:14:53,000 --> 00:14:56,000
这就是关于文档问答的全部内容。

185
00:14:56,000 --> 00:15:00,000
正如您可能已经注意到的那样，我们这里有许多不同的链条。

186
00:15:00,000 --> 00:15:12,000
因此，在下一节中，我们将介绍更好地了解所有这些链条内部究竟发生了什么的方法。